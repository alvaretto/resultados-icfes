#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M칩dulo de Chat de IA para An치lisis de Resultados ICFES
Instituci칩n Educativa Pedacito de Cielo

Este m칩dulo implementa un asistente conversacional de IA que ayuda a los usuarios
a interpretar y analizar los resultados del examen ICFES Saber 11.

Caracter칤sticas:
- Respuestas contextualizadas con datos reales
- Soporte para m칰ltiples proveedores LLM (Groq, Ollama, Together.ai)
- Memoria conversacional
- Streaming de respuestas
- Preguntas sugeridas

Autor: Sistema de An치lisis ICFES
Fecha: 2025-10-22
"""

import streamlit as st
import pandas as pd
from typing import Dict, List, Optional
import os

# ============================================================================
# CONFIGURACI칍N
# ============================================================================

# Configuraci칩n de modelos disponibles en Groq (cloud)
MODELOS_DISPONIBLES = {
    "llama-3.3-70b": "llama-3.3-70b-versatile",
    "llama-3.1-8b": "llama-3.1-8b-instant",
    "qwen3-32b": "qwen/qwen3-32b",
    "gpt-oss-120b": "openai/gpt-oss-120b"
}

# Configuraci칩n por defecto
MODELO_DEFAULT = "llama-3.3-70b"

# ============================================================================
# FUNCIONES DE INICIALIZACI칍N
# ============================================================================

def inicializar_chat():
    """
    Inicializa el estado del chat en Streamlit session_state
    """
    # Historial de mensajes
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []
    
    # Cliente LLM
    if "llm_client" not in st.session_state:
        st.session_state.llm_client = None
    
    # Configuraci칩n
    if "chat_config" not in st.session_state:
        st.session_state.chat_config = {
            "modelo": MODELO_DEFAULT,
            "temperatura": 0.7,
            "max_tokens": 2048
        }

def configurar_cliente_llm(modelo: str = "llama-3.3-70b"):
    """
    Configura el cliente Groq (cloud)

    Args:
        modelo: Nombre del modelo a usar (llama-3.3-70b, llama-3.1-8b, qwen3-32b, gpt-oss-120b)
    """
    try:
        from groq import Groq

        # Obtener API key de secrets o variables de entorno
        api_key = None
        if hasattr(st, 'secrets') and "GROQ_API_KEY" in st.secrets:
            api_key = st.secrets["GROQ_API_KEY"]
        else:
            api_key = os.getenv("GROQ_API_KEY")

        if not api_key:
            st.error("丘멆잺 No se encontr칩 la API key de Groq. Config칰rala en .streamlit/secrets.toml")
            return None

        client = Groq(api_key=api_key)
        st.session_state.llm_client = {
            "cliente": client,
            "modelo": MODELOS_DISPONIBLES[modelo]
        }
        return client

    except ImportError as e:
        st.error(f"丘멆잺 Error al importar librer칤a: {e}")
        st.info("游눠 Instala la dependencia: `pip install groq`")
        return None
    except Exception as e:
        st.error(f"丘멆잺 Error al configurar cliente Groq: {e}")
        return None

# ============================================================================
# FUNCIONES DE CONTEXTO
# ============================================================================

def construir_contexto_datos(df: pd.DataFrame, pagina_actual: str = "General", datos_2024: dict = None) -> str:
    """
    Construye el contexto con datos relevantes del DataFrame

    IMPORTANTE: Este contexto NO incluye informaci칩n personal identificable.
    Solo proporciona estad칤sticas agregadas y an치lisis generales.

    Args:
        df: DataFrame con los datos de estudiantes de 2025
        pagina_actual: Nombre de la p치gina/secci칩n actual
        datos_2024: Diccionario con estad칤sticas de 2024 (opcional)

    Returns:
        String con el contexto formateado (solo estad칤sticas agregadas)
    """
    if df is None or len(df) == 0:
        return "No hay datos disponibles."

    # PROTECCI칍N DE PRIVACIDAD: Crear copia del DataFrame sin columnas sensibles
    columnas_sensibles = ['Documento', 'N칰mero de Documento', 'C칠dula', 'ID',
                          'Nombre', 'Apellido', 'Nombres', 'Apellidos']
    df_seguro = df.copy()
    for col in columnas_sensibles:
        if col in df_seguro.columns:
            df_seguro = df_seguro.drop(columns=[col])

    contexto = f"""
# CONTEXTO DE DATOS ICFES - PEDACITO DE CIELO
丘멆잺 NOTA: Este contexto contiene SOLO estad칤sticas agregadas. NO incluye informaci칩n personal.

## P치gina actual: {pagina_actual}

## Estad칤sticas Generales 2025
- Total de estudiantes: {len(df_seguro)}
- Puntaje global promedio: {df_seguro['Puntaje Global'].mean():.0f} puntos
- Desviaci칩n est치ndar: {df_seguro['Puntaje Global'].std():.1f}
- Puntaje m칤nimo: {df_seguro['Puntaje Global'].min():.0f}
- Puntaje m치ximo: {df_seguro['Puntaje Global'].max():.0f}
"""

    # Agregar datos de 2024 si est치n disponibles
    if datos_2024 is not None and 'Institucional' in datos_2024:
        stats_2024 = datos_2024['Institucional']
        puntaje_2025 = df_seguro['Puntaje Global'].mean()
        puntaje_2024 = stats_2024['puntaje_global']
        cambio = puntaje_2025 - puntaje_2024
        cambio_pct = (cambio / puntaje_2024 * 100)

        contexto += f"""
## Estad칤sticas Generales 2024 (para comparaci칩n)
- Total de estudiantes: {stats_2024['estudiantes']}
- Puntaje global promedio: {puntaje_2024:.0f} puntos
- Desviaci칩n est치ndar: {stats_2024['desv_global']:.1f}

## Comparaci칩n 2024 vs 2025
- Cambio en puntaje global: {cambio:+.1f} puntos ({cambio_pct:+.1f}%)
- Interpretaci칩n: {"MEJOR칍" if cambio > 0 else "DISMINUY칍" if cambio < 0 else "SE MANTUVO IGUAL"}
"""

    contexto += "\n## Promedios por 츼rea de Conocimiento (2025)\n"

    areas = ['Lectura Cr칤tica', 'Matem치ticas', 'Sociales y Ciudadanas',
             'Ciencias Naturales', 'Ingl칠s']

    for area in areas:
        if area in df_seguro.columns:
            promedio_2025 = df_seguro[area].mean()
            desv = df_seguro[area].std()
            contexto += f"- {area}: {promedio_2025:.0f} puntos (픢={desv:.1f})"

            # Agregar comparaci칩n con 2024 si est치 disponible
            if datos_2024 is not None and 'Institucional' in datos_2024:
                areas_2024 = datos_2024['Institucional'].get('areas', {})
                if area in areas_2024:
                    promedio_2024 = areas_2024[area]['promedio']
                    cambio = promedio_2025 - promedio_2024
                    contexto += f" | 2024: {promedio_2024:.0f} | Cambio: {cambio:+.1f}"

            contexto += "\n"

    # Agregar datos por modelo educativo
    if 'Modelo' in df_seguro.columns:
        contexto += "\n## Comparaci칩n por Modelo Educativo\n"
        for modelo in df_seguro['Modelo'].unique():
            df_modelo = df_seguro[df_seguro['Modelo'] == modelo]
            promedio = df_modelo['Puntaje Global'].mean()
            contexto += f"- {modelo}: {promedio:.0f} puntos ({len(df_modelo)} estudiantes)\n"

    # Agregar datos por grupo
    if 'Grupo' in df_seguro.columns:
        contexto += "\n## Resultados por Grupo\n"
        for grupo in sorted(df_seguro['Grupo'].unique()):
            df_grupo = df_seguro[df_seguro['Grupo'] == grupo]
            promedio = df_grupo['Puntaje Global'].mean()
            contexto += f"- {grupo}: {promedio:.0f} puntos ({len(df_grupo)} estudiantes)\n"
    
    return contexto

def obtener_documentacion_icfes() -> str:
    """
    Retorna la documentaci칩n sobre interpretaci칩n de resultados ICFES
    """
    return """
# GU칈A DE INTERPRETACI칍N ICFES SABER 11

## Niveles de Desempe침o por Puntaje

### Insuficiente (0-35 puntos)
- El estudiante NO supera las preguntas de menor complejidad de la prueba
- Requiere refuerzo intensivo en competencias b치sicas del 치rea

### M칤nimo (36-50 puntos)
- El estudiante supera las preguntas de menor complejidad
- Necesita fortalecer competencias de nivel intermedio

### Satisfactorio (51-70 puntos)
- El estudiante supera las preguntas de complejidad media y baja
- Puede avanzar hacia el desarrollo de competencias avanzadas

### Avanzado (71-100 puntos)
- El estudiante supera las preguntas de mayor complejidad
- Mantener y profundizar en competencias de nivel superior

## 츼reas Evaluadas

### 1. Lectura Cr칤tica
Eval칰a la capacidad para comprender, interpretar y evaluar textos que pueden 
encontrarse tanto en la vida cotidiana como en 치mbitos acad칠micos no especializados.

### 2. Matem치ticas
Eval칰a las competencias matem치ticas que debe desarrollar un estudiante al 
finalizar el grado und칠cimo, relacionadas con el razonamiento cuantitativo.

### 3. Sociales y Ciudadanas
Eval칰a conocimientos y habilidades para comprender y analizar problemas sociales, 
as칤 como competencias ciudadanas.

### 4. Ciencias Naturales
Eval칰a competencias para comprender y usar conocimientos de las ciencias naturales 
en la soluci칩n de problemas.

### 5. Ingl칠s
Eval칰a la competencia comunicativa en lengua extranjera (ingl칠s), enfoc치ndose 
en comprensi칩n lectora.

## Puntaje Global
- Es la SUMA de los puntajes de las 5 치reas
- Rango: 0 a 500 puntos
- Promedio nacional t칤pico: ~250 puntos
- Un buen puntaje institucional: 260-280 puntos
- Un excelente puntaje institucional: 280+ puntos

## Interpretaci칩n de Avances
- Avance de +5 puntos o m치s: Mejora significativa
- Avance de +1 a +4 puntos: Mejora moderada
- Sin cambio (0): Estabilidad
- Retroceso de -1 a -4 puntos: Disminuci칩n moderada
- Retroceso de -5 puntos o m치s: Disminuci칩n significativa

## Desviaci칩n Est치ndar
- Mide la dispersi칩n de los resultados
- Menor desviaci칩n = Mayor homogeneidad (todos los estudiantes con resultados similares)
- Mayor desviaci칩n = Mayor heterogeneidad (resultados muy variados entre estudiantes)
"""

def construir_prompt_sistema() -> str:
    """
    Construye el prompt del sistema para el asistente de IA
    """
    return """
Eres un asistente experto en an치lisis de resultados ICFES Saber 11 para la
Instituci칩n Educativa Pedacito de Cielo.

INSTRUCCIONES IMPORTANTES:
1. Responde SIEMPRE en espa침ol de forma clara y pedag칩gica
2. Usa los datos del contexto proporcionado para fundamentar tus respuestas
3. Si no tienes informaci칩n suficiente en el contexto, ind칤calo claramente
4. Proporciona interpretaciones educativas 칰tiles y constructivas
5. Usa emojis ocasionalmente para hacer las respuestas m치s amigables
6. S칠 conciso pero completo en tus explicaciones
7. Cuando hables de avances o retrocesos, contextualiza su significado
8. Recuerda que NO se deben comparar promedios entre 치reas diferentes
9. Cada 치rea se analiza de forma independiente

丘멆잺 PRIVACIDAD Y PROTECCI칍N DE DATOS - CR칈TICO:
- NUNCA reveles, menciones o proporciones n칰meros de documento de identidad
- NUNCA reveles informaci칩n personal identificable de estudiantes individuales
- Si te preguntan por datos de un estudiante espec칤fico, responde: "Por pol칤ticas de privacidad, no puedo proporcionar informaci칩n personal de estudiantes individuales. Puedo ayudarte con estad칤sticas agregadas y an치lisis generales."
- Solo proporciona estad칤sticas agregadas y an치lisis generales
- Protege la confidencialidad de los datos en todo momento

FORMATO DE RESPUESTAS:
- Usa listas con vi침etas para informaci칩n estructurada
- Destaca datos importantes con **negritas**
- Incluye interpretaciones pedag칩gicas cuando sea relevante
- Sugiere acciones concretas cuando sea apropiado

TONO:
- Profesional pero accesible
- Constructivo y orientado a la mejora
- Emp치tico con los desaf칤os educativos
"""

# ============================================================================
# FUNCIONES DE PROTECCI칍N DE PRIVACIDAD
# ============================================================================

def filtrar_informacion_sensible(texto: str) -> str:
    """
    Filtra informaci칩n sensible de la respuesta del LLM

    Args:
        texto: Texto de respuesta del LLM

    Returns:
        Texto filtrado sin informaci칩n sensible
    """
    import re

    # Patrones de n칰meros de documento (c칠dulas colombianas t칤picamente 7-10 d칤gitos)
    # Buscar secuencias de 7+ d칤gitos que podr칤an ser documentos
    patron_documento = r'\b\d{7,10}\b'

    # Si se encuentra un patr칩n de documento, reemplazarlo
    if re.search(patron_documento, texto):
        texto = re.sub(patron_documento, '[INFORMACI칍N PROTEGIDA]', texto)
        texto += "\n\n丘멆잺 **Nota:** Por pol칤ticas de privacidad, algunos datos personales han sido ocultados."

    return texto

# ============================================================================
# FUNCIONES DE GENERACI칍N DE RESPUESTAS
# ============================================================================

def generar_respuesta(prompt: str, contexto: str = "") -> str:
    """
    Genera una respuesta usando el modelo LLM configurado
    
    Args:
        prompt: Pregunta del usuario
        contexto: Contexto adicional con datos
    
    Returns:
        Respuesta generada por el modelo
    """
    if st.session_state.llm_client is None:
        return "丘멆잺 El cliente LLM no est치 configurado. Por favor, configura la API key."
    
    try:
        client_info = st.session_state.llm_client
        
        # Construir mensajes
        messages = [
            {"role": "system", "content": construir_prompt_sistema()},
        ]
        
        # Agregar contexto si existe
        if contexto:
            messages.append({
                "role": "system", 
                "content": f"CONTEXTO CON DATOS ACTUALES:\n{contexto}\n\n{obtener_documentacion_icfes()}"
            })
        
        # Agregar historial de conversaci칩n (칰ltimos 5 mensajes)
        for msg in st.session_state.chat_messages[-5:]:
            messages.append({"role": msg["role"], "content": msg["content"]})
        
        # Agregar pregunta actual
        messages.append({"role": "user", "content": prompt})

        # Generar respuesta con Groq
        response = client_info["cliente"].chat.completions.create(
            model=client_info["modelo"],
            messages=messages,
            temperature=st.session_state.chat_config["temperatura"],
            max_tokens=st.session_state.chat_config["max_tokens"],
            stream=False
        )

        # Obtener respuesta y filtrar informaci칩n sensible
        respuesta_raw = response.choices[0].message.content
        respuesta_filtrada = filtrar_informacion_sensible(respuesta_raw)

        return respuesta_filtrada

    except Exception as e:
        return f"丘멆잺 Error al generar respuesta: {str(e)}"

# ============================================================================
# INTERFAZ DE USUARIO
# ============================================================================

def mostrar_preguntas_sugeridas() -> Optional[str]:
    """
    Muestra botones con preguntas sugeridas comunes
    
    Returns:
        Pregunta seleccionada o None
    """
    st.markdown("#### 游눠 Preguntas sugeridas:")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("游늵 Avance institucional", use_container_width=True):
            return "쮺칩mo mejor칩 el puntaje global de la instituci칩n entre 2024 y 2025?"
    
    with col2:
        if st.button("游닄 츼rea m치s fuerte", use_container_width=True):
            return "쮺u치l es el 치rea de conocimiento con mejor desempe침o en 2025?"
    
    with col3:
        if st.button("游꿢 츼reas a mejorar", use_container_width=True):
            return "쮼n qu칠 치reas debemos enfocarnos para mejorar los resultados?"
    
    col4, col5, col6 = st.columns(3)
    
    with col4:
        if st.button("游댃 Comparar modelos", use_container_width=True):
            return "쮺칩mo se comparan los resultados entre Aula Regular y Modelo Flexible?"
    
    with col5:
        if st.button("游늳 Interpretar puntajes", use_container_width=True):
            return "쮺칩mo interpreto los puntajes y niveles de desempe침o?"
    
    with col6:
        if st.button("游눩 Recomendaciones", use_container_width=True):
            return "쯈u칠 estrategias pedag칩gicas recomiendas para mejorar?"
    
    return None

def mostrar_chat(df: pd.DataFrame = None, pagina_actual: str = "General", datos_2024: dict = None):
    """
    Muestra la interfaz completa del chat de IA

    Args:
        df: DataFrame con los datos de 2025
        pagina_actual: Nombre de la p치gina/secci칩n actual
        datos_2024: Diccionario con estad칤sticas de 2024 (opcional)
    """
    # Inicializar
    inicializar_chat()
    
    # Configurar cliente si no existe
    if st.session_state.llm_client is None:
        configurar_cliente_llm()
    
    # Header del chat
    st.markdown("### 游뱄 Asistente de IA - Resultados ICFES")
    st.markdown("*Pregunta sobre los datos, interpretaciones y recomendaciones pedag칩gicas*")
    
    # Preguntas sugeridas
    pregunta_sugerida = mostrar_preguntas_sugeridas()
    
    st.markdown("---")
    
    # Mostrar historial de mensajes
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input del usuario
    prompt = st.chat_input("Escribe tu pregunta aqu칤...")
    
    # Si hay pregunta sugerida, usarla
    if pregunta_sugerida:
        prompt = pregunta_sugerida
    
    # Procesar pregunta
    if prompt:
        # Agregar mensaje del usuario
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Construir contexto
        contexto = ""
        if df is not None:
            contexto = construir_contexto_datos(df, pagina_actual, datos_2024)
        
        # Generar y mostrar respuesta
        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                response = generar_respuesta(prompt, contexto)
                st.markdown(response)
        
        # Agregar respuesta al historial
        st.session_state.chat_messages.append({"role": "assistant", "content": response})
        
        # Rerun para actualizar la interfaz
        st.rerun()

# ============================================================================
# FUNCI칍N PRINCIPAL PARA TESTING
# ============================================================================

if __name__ == "__main__":
    st.set_page_config(page_title="Chat IA ICFES", page_icon="游뱄", layout="wide")
    
    st.title("游뱄 Chat de IA - Resultados ICFES")
    st.markdown("---")
    
    # Mostrar chat sin datos (modo demo)
    mostrar_chat()

