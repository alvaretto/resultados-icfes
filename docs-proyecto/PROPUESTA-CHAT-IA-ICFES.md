# ü§ñ Propuesta: Chat de IA Integrado para An√°lisis de Resultados ICFES

**Fecha:** 22 de octubre de 2025  
**Proyecto:** Resultados ICFES - Pedacito de Cielo  
**Objetivo:** Implementar un asistente de IA conversacional para ayudar a usuarios a interpretar y analizar datos del ICFES

---

## üìã √çndice

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [An√°lisis de Modelos LLM Open Source](#an√°lisis-de-modelos-llm-open-source)
3. [Arquitectura Propuesta](#arquitectura-propuesta)
4. [Opciones de Hosting](#opciones-de-hosting)
5. [Plan de Implementaci√≥n](#plan-de-implementaci√≥n)
6. [Estimaci√≥n de Costos](#estimaci√≥n-de-costos)
7. [Recomendaciones](#recomendaciones)

---

## üéØ Resumen Ejecutivo

### Objetivo
Integrar un chat de IA en la aplicaci√≥n Streamlit de Resultados ICFES que permita a los usuarios:
- Hacer preguntas sobre los datos y estad√≠sticas mostrados
- Obtener interpretaciones pedag√≥gicas de los resultados
- Recibir explicaciones sobre puntajes y √°reas evaluadas
- Comparar resultados entre a√±os, modelos educativos y grupos

### Requisitos T√©cnicos Clave
‚úÖ **Modelo open source** (c√≥digo abierto)  
‚úÖ **Ventana de contexto grande** (m√≠nimo 32K, idealmente 128K+ tokens)  
‚úÖ **Soporte de espa√±ol de alta calidad**  
‚úÖ **API gratuita o econ√≥mica**  
‚úÖ **Integraci√≥n con Streamlit**

---

## üîç An√°lisis de Modelos LLM Open Source

### 1. **DeepSeek V3 / DeepSeek R1** ‚≠ê RECOMENDADO

**Caracter√≠sticas:**
- **Par√°metros:** 671B (Mixture of Experts - 37B activos)
- **Ventana de contexto:** 128K tokens
- **Espa√±ol:** Excelente calidad (entrenado multiling√ºe)
- **Rendimiento:** Comparable a GPT-4 en benchmarks
- **Licencia:** MIT (completamente open source)

**Ventajas:**
- ‚úÖ Mejor modelo open source disponible actualmente (2025)
- ‚úÖ Razonamiento avanzado (DeepSeek R1)
- ‚úÖ Excelente en espa√±ol
- ‚úÖ APIs gratuitas disponibles (Groq, Together.ai, OpenRouter)
- ‚úÖ Puede ejecutarse localmente con Ollama

**Desventajas:**
- ‚ö†Ô∏è Requiere ~40GB RAM para ejecuci√≥n local (versi√≥n cuantizada)
- ‚ö†Ô∏è Versi√≥n completa requiere m√∫ltiples GPUs

**APIs Gratuitas:**
- **Groq:** Gratis con l√≠mites generosos (14,400 req/d√≠a)
- **Together.ai:** $1M tokens gratis al mes
- **OpenRouter:** Versi√≥n gratuita disponible
- **Scaleway:** 1M tokens gratis

---

### 2. **Llama 3.3 70B** ‚≠ê ALTERNATIVA S√ìLIDA

**Caracter√≠sticas:**
- **Par√°metros:** 70B
- **Ventana de contexto:** 128K tokens
- **Espa√±ol:** Muy buena calidad
- **Rendimiento:** Excelente en tareas generales
- **Licencia:** Llama 3 Community License (open source)

**Ventajas:**
- ‚úÖ Modelo probado y confiable de Meta
- ‚úÖ Amplio soporte en APIs gratuitas
- ‚úÖ Buena documentaci√≥n y comunidad
- ‚úÖ Ejecutable localmente con Ollama

**Desventajas:**
- ‚ö†Ô∏è Requiere ~40GB RAM para ejecuci√≥n local
- ‚ö†Ô∏è Espa√±ol ligeramente inferior a DeepSeek

**APIs Gratuitas:**
- **Groq:** Gratis (ultra r√°pido)
- **Together.ai:** Gratis
- **OpenRouter:** Gratis
- **NVIDIA NIM:** Gratis con l√≠mites

---

### 3. **Qwen 2.5 / Qwen 3** üåü MEJOR PARA ESPA√ëOL

**Caracter√≠sticas:**
- **Par√°metros:** 7B, 14B, 32B, 72B, 235B (MoE)
- **Ventana de contexto:** 128K tokens (hasta 1M en Qwen 2.5)
- **Espa√±ol:** Excelente (mejor que Llama en idiomas no ingleses)
- **Rendimiento:** Muy competitivo
- **Licencia:** Apache 2.0 (completamente open source)

**Ventajas:**
- ‚úÖ Excelente en espa√±ol y multiling√ºe
- ‚úÖ Versiones peque√±as (7B, 14B) ejecutables en hardware modesto
- ‚úÖ Ventana de contexto masiva (hasta 1M tokens)
- ‚úÖ Muy eficiente

**Desventajas:**
- ‚ö†Ô∏è Menos APIs gratuitas que Llama/DeepSeek
- ‚ö†Ô∏è Documentaci√≥n principalmente en ingl√©s/chino

**APIs Gratuitas:**
- **Alibaba Cloud:** Gratis con l√≠mites
- **Together.ai:** Disponible
- **OpenRouter:** Disponible

---

### 4. **Mixtral 8x7B / 8x22B**

**Caracter√≠sticas:**
- **Par√°metros:** 47B total (8 expertos x 7B, 12.9B activos)
- **Ventana de contexto:** 32K tokens
- **Espa√±ol:** Buena calidad
- **Licencia:** Apache 2.0

**Ventajas:**
- ‚úÖ Eficiente (MoE)
- ‚úÖ Bueno en espa√±ol
- ‚úÖ Ejecutable localmente

**Desventajas:**
- ‚ö†Ô∏è Ventana de contexto menor (32K)
- ‚ö†Ô∏è Superado por modelos m√°s recientes

---

## üèóÔ∏è Arquitectura Propuesta

### Componentes del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    APLICACI√ìN STREAMLIT                      ‚îÇ
‚îÇ                  (streamlit_app.py)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    M√ìDULO CHAT IA                            ‚îÇ
‚îÇ                  (chat_ia_icfes.py)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Interfaz de chat (st.chat_message)                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Gesti√≥n de historial de conversaci√≥n              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Procesamiento de consultas                         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  CAPA DE CONTEXTO (RAG)                      ‚îÇ
‚îÇ                  (contexto_icfes.py)                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Extracci√≥n de datos relevantes                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Construcci√≥n de contexto din√°mico                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Embeddings de documentaci√≥n ICFES                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Recuperaci√≥n de informaci√≥n (RAG)                  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLIENTE LLM                               ‚îÇ
‚îÇ                  (llm_client.py)                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Conexi√≥n a API (Groq/Together.ai/Ollama)          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Gesti√≥n de prompts                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Streaming de respuestas                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Manejo de errores y reintentos                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FUENTES DE DATOS                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ DataFrames de Pandas (datos 2024/2025)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Archivos Excel (resultados estudiantes)           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Documentaci√≥n ICFES (MD files)                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Estad√≠sticas calculadas en tiempo real            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Interacci√≥n

1. **Usuario hace una pregunta** en el chat de Streamlit
2. **M√≥dulo Chat IA** recibe la consulta y la procesa
3. **Capa de Contexto (RAG)** extrae datos relevantes:
   - Estad√≠sticas actuales de la p√°gina
   - Datos hist√≥ricos (2024 vs 2025)
   - Documentaci√≥n ICFES relevante
4. **Cliente LLM** env√≠a la consulta + contexto al modelo
5. **Modelo LLM** genera respuesta en espa√±ol
6. **Respuesta se muestra** en el chat con streaming

---

## üåê Opciones de Hosting

### Opci√≥n 1: **API Gratuita (Groq)** ‚≠ê RECOMENDADO PARA INICIO

**Proveedor:** Groq Cloud  
**Modelo:** DeepSeek R1 o Llama 3.3 70B  
**Costo:** Gratis  
**L√≠mites:** 14,400 requests/d√≠a, 30 req/min

**Ventajas:**
- ‚úÖ Cero costo
- ‚úÖ Ultra r√°pido (hardware especializado)
- ‚úÖ F√°cil integraci√≥n
- ‚úÖ No requiere infraestructura local

**Desventajas:**
- ‚ö†Ô∏è L√≠mites de uso
- ‚ö†Ô∏è Dependencia de servicio externo
- ‚ö†Ô∏è Datos enviados a terceros

**Implementaci√≥n:**
```python
from groq import Groq

client = Groq(api_key="tu_api_key_gratis")
response = client.chat.completions.create(
    model="deepseek-r1-distill-llama-70b",
    messages=[{"role": "user", "content": "pregunta"}],
    temperature=0.7,
    max_tokens=2048
)
```

---

### Opci√≥n 2: **Ollama Local** üè† RECOMENDADO PARA PRIVACIDAD

**Proveedor:** Ollama (local)  
**Modelo:** Qwen 2.5 14B o Llama 3.3 70B  
**Costo:** Gratis (hardware propio)  
**Requisitos:** 16GB+ RAM, GPU opcional

**Ventajas:**
- ‚úÖ 100% privado (datos no salen del servidor)
- ‚úÖ Sin l√≠mites de uso
- ‚úÖ Sin costos recurrentes
- ‚úÖ Control total

**Desventajas:**
- ‚ö†Ô∏è Requiere hardware adecuado
- ‚ö†Ô∏è M√°s lento que APIs cloud
- ‚ö†Ô∏è Mantenimiento local

**Implementaci√≥n:**
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo
ollama pull qwen2.5:14b

# Ejecutar
ollama serve
```

```python
import ollama

response = ollama.chat(
    model='qwen2.5:14b',
    messages=[{'role': 'user', 'content': 'pregunta'}]
)
```

---

### Opci√≥n 3: **Together.ai** üí∞ ECON√ìMICO Y ESCALABLE

**Proveedor:** Together.ai  
**Modelo:** DeepSeek V3, Llama 3.3, Qwen 2.5  
**Costo:** $1M tokens gratis/mes, luego $0.20-0.60 por 1M tokens  
**L√≠mites:** Muy generosos

**Ventajas:**
- ‚úÖ Tier gratuito generoso
- ‚úÖ M√∫ltiples modelos disponibles
- ‚úÖ Buena velocidad
- ‚úÖ Escalable

**Desventajas:**
- ‚ö†Ô∏è Costo despu√©s del tier gratuito
- ‚ö†Ô∏è Datos enviados a terceros

---

## üì¶ Frameworks y Librer√≠as

### LangChain vs LlamaIndex

| Caracter√≠stica | LangChain | LlamaIndex |
|---------------|-----------|------------|
| **Prop√≥sito** | Framework general para LLMs | Especializado en RAG |
| **Curva de aprendizaje** | Moderada | M√°s simple para RAG |
| **Flexibilidad** | Muy alta | Enfocada en datos |
| **Documentaci√≥n** | Extensa | Buena |
| **Integraci√≥n Streamlit** | Excelente | Excelente |
| **Recomendaci√≥n** | ‚úÖ Para proyecto complejo | ‚úÖ Para RAG simple |

### Recomendaci√≥n: **LangChain** 

**Razones:**
- M√°s flexible para futuras expansiones
- Mejor integraci√≥n con m√∫ltiples proveedores
- Excelente para gesti√≥n de memoria conversacional
- Amplia comunidad y recursos

**Dependencias:**
```txt
langchain>=0.1.0
langchain-groq>=0.0.1  # Para Groq
langchain-community>=0.0.1
streamlit>=1.32.0
pandas>=2.2.0
```

---

## üìù Plan de Implementaci√≥n

### Fase 1: Configuraci√≥n B√°sica (1-2 d√≠as)

**Tareas:**
1. ‚úÖ Crear cuenta en Groq (API gratuita)
2. ‚úÖ Instalar dependencias necesarias
3. ‚úÖ Crear m√≥dulo b√°sico de chat en Streamlit
4. ‚úÖ Implementar cliente LLM con Groq

**Entregables:**
- Chat funcional con respuestas b√°sicas
- Integraci√≥n con DeepSeek R1 o Llama 3.3

**C√≥digo ejemplo:**
```python
# chat_ia_icfes.py
import streamlit as st
from groq import Groq

def inicializar_chat():
    """Inicializa el chat de IA"""
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "groq_client" not in st.session_state:
        st.session_state.groq_client = Groq(
            api_key=st.secrets["GROQ_API_KEY"]
        )

def mostrar_chat():
    """Muestra la interfaz del chat"""
    st.markdown("### ü§ñ Asistente de IA - Resultados ICFES")

    # Mostrar historial
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Input del usuario
    if prompt := st.chat_input("Pregunta sobre los resultados ICFES..."):
        # Agregar mensaje del usuario
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Generar respuesta
        with st.chat_message("assistant"):
            response = generar_respuesta(prompt)
            st.markdown(response)

        st.session_state.messages.append({"role": "assistant", "content": response})

def generar_respuesta(prompt):
    """Genera respuesta usando Groq"""
    client = st.session_state.groq_client

    response = client.chat.completions.create(
        model="deepseek-r1-distill-llama-70b",
        messages=[
            {"role": "system", "content": "Eres un asistente experto en an√°lisis de resultados ICFES Saber 11. Respondes en espa√±ol de forma clara y pedag√≥gica."},
            *st.session_state.messages,
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=2048,
        stream=False
    )

    return response.choices[0].message.content
```

---

### Fase 2: Integraci√≥n con Datos (2-3 d√≠as)

**Tareas:**
1. ‚úÖ Crear m√≥dulo de contexto RAG
2. ‚úÖ Extraer datos relevantes de DataFrames
3. ‚úÖ Construir prompts con contexto din√°mico
4. ‚úÖ Implementar recuperaci√≥n de documentaci√≥n ICFES

**Entregables:**
- Chat que responde con datos reales
- Contexto din√°mico basado en la p√°gina actual

**C√≥digo ejemplo:**
```python
# contexto_icfes.py
import pandas as pd
from typing import Dict, List

def construir_contexto_datos(df: pd.DataFrame, pagina_actual: str) -> str:
    """Construye contexto con datos relevantes"""

    contexto = f"""
# CONTEXTO DE DATOS ICFES

## P√°gina actual: {pagina_actual}

## Estad√≠sticas Generales 2025
- Total estudiantes: {len(df)}
- Puntaje global promedio: {df['Puntaje Global'].mean():.0f}
- Desviaci√≥n est√°ndar: {df['Puntaje Global'].std():.1f}

## Promedios por √Årea
"""

    areas = ['Lectura Cr√≠tica', 'Matem√°ticas', 'Sociales y Ciudadanas',
             'Ciencias Naturales', 'Ingl√©s']

    for area in areas:
        if area in df.columns:
            promedio = df[area].mean()
            contexto += f"- {area}: {promedio:.0f}\n"

    # Agregar datos por modelo
    contexto += "\n## Comparaci√≥n por Modelo Educativo\n"
    for modelo in df['Modelo'].unique():
        df_modelo = df[df['Modelo'] == modelo]
        promedio = df_modelo['Puntaje Global'].mean()
        contexto += f"- {modelo}: {promedio:.0f} ({len(df_modelo)} estudiantes)\n"

    return contexto

def construir_contexto_comparativo(datos_2024: Dict, datos_2025: Dict) -> str:
    """Construye contexto comparativo 2024 vs 2025"""

    contexto = """
# COMPARACI√ìN 2024 vs 2025

## Puntaje Global Institucional
"""

    puntaje_2024 = datos_2024['Institucional']['puntaje_global']
    puntaje_2025 = datos_2025['puntaje_global']
    avance = puntaje_2025 - puntaje_2024

    contexto += f"- 2024: {puntaje_2024}\n"
    contexto += f"- 2025: {puntaje_2025}\n"
    contexto += f"- Avance: {avance:+d} puntos\n"

    return contexto

def obtener_documentacion_icfes() -> str:
    """Retorna documentaci√≥n sobre interpretaci√≥n ICFES"""

    return """
# GU√çA DE INTERPRETACI√ìN ICFES

## Niveles de Desempe√±o
- **Insuficiente (0-35)**: No supera preguntas de menor complejidad
- **M√≠nimo (36-50)**: Supera preguntas de menor complejidad
- **Satisfactorio (51-70)**: Supera preguntas de complejidad media y baja
- **Avanzado (71-100)**: Supera preguntas de mayor complejidad

## √Åreas Evaluadas
1. **Lectura Cr√≠tica**: Comprensi√≥n, interpretaci√≥n y evaluaci√≥n de textos
2. **Matem√°ticas**: Razonamiento cuantitativo y resoluci√≥n de problemas
3. **Sociales y Ciudadanas**: Pensamiento social y competencias ciudadanas
4. **Ciencias Naturales**: Indagaci√≥n y explicaci√≥n de fen√≥menos
5. **Ingl√©s**: Comprensi√≥n lectora en lengua extranjera

## Puntaje Global
- Suma de los puntajes de las 5 √°reas
- Rango: 0 a 500 puntos
- Promedio nacional t√≠pico: 250 puntos
"""
```

---

### Fase 3: Mejoras Avanzadas (3-5 d√≠as)

**Tareas:**
1. ‚úÖ Implementar memoria conversacional
2. ‚úÖ Agregar streaming de respuestas
3. ‚úÖ Crear prompts especializados por tipo de consulta
4. ‚úÖ Implementar cach√© de respuestas comunes
5. ‚úÖ Agregar botones de preguntas sugeridas

**Entregables:**
- Chat con memoria de conversaci√≥n
- Respuestas en tiempo real (streaming)
- Experiencia de usuario mejorada

**C√≥digo ejemplo:**
```python
# chat_avanzado.py
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_groq import ChatGroq

def crear_cadena_conversacional(contexto: str):
    """Crea cadena conversacional con LangChain"""

    llm = ChatGroq(
        model="deepseek-r1-distill-llama-70b",
        temperature=0.7,
        max_tokens=2048,
        groq_api_key=st.secrets["GROQ_API_KEY"]
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    system_prompt = f"""
Eres un asistente experto en an√°lisis de resultados ICFES Saber 11 para la
Instituci√≥n Educativa Pedacito de Cielo.

CONTEXTO ACTUAL:
{contexto}

INSTRUCCIONES:
- Responde SIEMPRE en espa√±ol
- Usa los datos del contexto para responder
- S√© claro, conciso y pedag√≥gico
- Si no tienes informaci√≥n suficiente, ind√≠calo
- Proporciona interpretaciones educativas √∫tiles
- Usa emojis para hacer las respuestas m√°s amigables
"""

    return llm, memory, system_prompt

def mostrar_preguntas_sugeridas():
    """Muestra botones con preguntas comunes"""

    st.markdown("#### üí° Preguntas sugeridas:")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üìä ¬øC√≥mo mejor√≥ la instituci√≥n?"):
            return "¬øC√≥mo mejor√≥ el puntaje global de la instituci√≥n entre 2024 y 2025?"

    with col2:
        if st.button("üìö ¬øCu√°l es el √°rea m√°s fuerte?"):
            return "¬øCu√°l es el √°rea de conocimiento con mejor desempe√±o?"

    with col3:
        if st.button("üéØ ¬øQu√© √°reas mejorar?"):
            return "¬øEn qu√© √°reas debemos enfocarnos para mejorar?"

    return None
```

---

### Fase 4: Optimizaci√≥n y Testing (2-3 d√≠as)

**Tareas:**
1. ‚úÖ Optimizar prompts para mejor calidad
2. ‚úÖ Implementar manejo de errores robusto
3. ‚úÖ Agregar logging y monitoreo
4. ‚úÖ Testing con usuarios reales
5. ‚úÖ Documentaci√≥n de uso

**Entregables:**
- Sistema estable y optimizado
- Documentaci√≥n completa
- Gu√≠a de usuario

---

## üí∞ Estimaci√≥n de Costos

### Opci√≥n 1: Groq (Gratis)
- **Costo mensual:** $0
- **L√≠mites:** 14,400 requests/d√≠a
- **Estimaci√≥n de uso:** ~500 requests/d√≠a (suficiente para instituci√≥n)
- **Costo anual:** $0

### Opci√≥n 2: Together.ai (Freemium)
- **Tier gratuito:** 1M tokens/mes
- **Estimaci√≥n de uso:** ~200K tokens/mes
- **Costo mensual:** $0 (dentro del tier gratuito)
- **Si se excede:** ~$0.20-0.60 por 1M tokens adicionales
- **Costo anual estimado:** $0-50

### Opci√≥n 3: Ollama Local
- **Costo inicial:** $0 (si ya tienes hardware)
- **Hardware recomendado:**
  - CPU: 8+ cores
  - RAM: 32GB (para Llama 3.3 70B) o 16GB (para Qwen 2.5 14B)
  - GPU: Opcional (NVIDIA con 8GB+ VRAM acelera 5-10x)
- **Costo mensual:** $0 (solo electricidad ~$5-10)
- **Costo anual:** $60-120 (electricidad)

### Recomendaci√≥n de Costos

**Para comenzar:** Groq (gratis)
**Para producci√≥n:** Together.ai o Ollama local
**Para m√°xima privacidad:** Ollama local

---

## üéØ Recomendaciones Finales

### Recomendaci√≥n Principal: **Enfoque H√≠brido**

1. **Fase Inicial (Mes 1-2):**
   - Usar **Groq** con **DeepSeek R1**
   - Validar funcionalidad y aceptaci√≥n de usuarios
   - Costo: $0

2. **Fase de Producci√≥n (Mes 3+):**
   - Migrar a **Ollama local** con **Qwen 2.5 14B**
   - Mantener Groq como backup
   - Costo: ~$10/mes (electricidad)

3. **Escalamiento (Si es necesario):**
   - Considerar **Together.ai** para mayor capacidad
   - O actualizar hardware para Llama 3.3 70B local

### Modelo Recomendado por Caso de Uso

| Caso de Uso | Modelo Recomendado | Raz√≥n |
|-------------|-------------------|-------|
| **Prototipo r√°pido** | DeepSeek R1 (Groq) | Gratis, r√°pido, excelente calidad |
| **Producci√≥n privada** | Qwen 2.5 14B (Ollama) | Privacidad, espa√±ol excelente, eficiente |
| **M√°ximo rendimiento** | Llama 3.3 70B (Ollama) | Mejor calidad, requiere m√°s hardware |
| **Escalabilidad** | DeepSeek V3 (Together.ai) | Balance costo/rendimiento |

### Pr√≥ximos Pasos Inmediatos

1. ‚úÖ **Crear cuenta en Groq** (5 minutos)
2. ‚úÖ **Instalar dependencias** (10 minutos)
3. ‚úÖ **Implementar chat b√°sico** (2-3 horas)
4. ‚úÖ **Probar con datos reales** (1 hora)
5. ‚úÖ **Iterar y mejorar** (continuo)

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial
- **Groq:** https://console.groq.com/docs
- **LangChain:** https://python.langchain.com/docs
- **Streamlit Chat:** https://docs.streamlit.io/develop/api-reference/chat
- **Ollama:** https://ollama.com/docs

### Tutoriales Recomendados
- [Building RAG with LangChain and Streamlit](https://blog.streamlit.io/build-a-real-time-rag-chatbot-google-drive-sharepoint/)
- [Groq + LangChain Integration](https://console.groq.com/docs/langchain)
- [Ollama Local Setup](https://ollama.com/blog)

### Comunidades
- r/LocalLLaMA (Reddit)
- LangChain Discord
- Streamlit Community Forum

---

## ‚úÖ Checklist de Implementaci√≥n

### Configuraci√≥n Inicial
- [ ] Crear cuenta en Groq
- [ ] Obtener API key
- [ ] Configurar secrets en Streamlit
- [ ] Instalar dependencias

### Desarrollo
- [ ] Crear m√≥dulo chat_ia_icfes.py
- [ ] Implementar interfaz de chat
- [ ] Crear m√≥dulo contexto_icfes.py
- [ ] Integrar con datos existentes
- [ ] Implementar cliente LLM

### Testing
- [ ] Probar con preguntas b√°sicas
- [ ] Validar respuestas con datos reales
- [ ] Testing con usuarios
- [ ] Optimizar prompts

### Despliegue
- [ ] Documentar uso
- [ ] Agregar a aplicaci√≥n principal
- [ ] Configurar en Streamlit Cloud
- [ ] Monitorear uso

---

## üìû Contacto y Soporte

Para dudas sobre la implementaci√≥n:
- Revisar documentaci√≥n oficial de cada herramienta
- Consultar ejemplos en el repositorio
- Buscar en comunidades especializadas

---

**Documento creado:** 22 de octubre de 2025
**√öltima actualizaci√≥n:** 22 de octubre de 2025
**Versi√≥n:** 1.0


